{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Download LLM \n",
    "1. Meta 홈페이지 \n",
    "2. huggingface CLI \n",
    "3. transformers\n",
    "4. snapshot download\n",
    "5. Ollama -> `test_llama3_ollama.ipynb` 파일 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. huggingface CLI Downloads  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n",
    "  \n",
    "- Visit one of the repos, for example [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n",
    "- Read and accept the license. Once your request is approved, you'll be granted access to all the Llama 3 models. Note that requests used to take up to one hour to get processed.\n",
    "- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you   \n",
    "- `pip install huggingface-hub`:  \n",
    "- `huggingface-cli login` \n",
    "-  ```\n",
    "    huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3-8B-Instruct\n",
    "    ```\n",
    "- 이와 같이 원본 라마3 체크포인트를 다운 받은 후, 간단히 pipeline방법을 사용하여 inference가능\n",
    "  - [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights\n",
    "- pipeline은 사용방법이 직관적이고 매우 쉬운 반면, custom할 수 있는 폭이 훨신 좁아서 여러가지 시도를 하고싶다면 AutoModelForCausalLM.from_pretrained()로 불러오는 것이 좋다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model = model_id,\n",
    "    model_kwargs = {\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipelone.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens = 256,\n",
    "    eos_token_id = terminators,\n",
    "    do_sample=True,\n",
    "    temperature = 0.6,\n",
    "    top_p=0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transformers Library 사용하여 Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pip install accelerate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 모델 로드\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# 모델 저장\n",
    "save_dir = \"./llama_3_8b_instruct\"\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir)\n",
    "\n",
    "\n",
    "robot_command = \"\"\"The robot is equipped with wheels and a manipulator arm.\n",
    "When given the command \"Water, please\" to the robot:\n",
    "\n",
    "1. What locations does the robot need to move to in order to execute the command? Please provide at least three common locations where that is typically found.\n",
    "2. What actions does the robot need to perform?\"\"\" \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": robot_command}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Snapshot Download\n",
    "- snapshot_download 함수는 Hugging Face Hub에서 특정 모델이나 데이터셋의 스냅샷을 로컬 디렉토리에 다운로드하는 데 사용됨. 이 함수는 특히 모델 체크포인트, 구성 파일, 토크나이저 파일 등을 포함한 전체 리포지토리를 다운로드하는 데 유용함\n",
    "- llama.cpp 를 사용하기 위한 모델 다운로드를 위해 해당 방법 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetson/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jetson/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 14 files: 100%|██████████| 14/14 [00:22<00:00,  1.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jetson/llamaR/Llama3-Playground/models/llama_3_8b_instruct'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", local_dir=\"./models/llama_3_8b_instruct\",\n",
    "                  local_dir_use_symlinks=False, ignore_patterns=[\"original/*\"],)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e8158a314f02ec0c390a43a5045b72062fab839463d13d14c6a1f932c8ca7c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 ('nuplan': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
