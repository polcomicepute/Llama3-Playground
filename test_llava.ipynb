{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAVA-NEXT (v1.6)\n",
    "\n",
    "- 입력 이미지 x4 res 증가 : 672x672, 336x1344, 1344x336 \n",
    "- liuhaotian/llava-v1.6-mistral-7b\n",
    "- liuhaotian/llava-v1.6-vicuna-7b\n",
    "- llava-hf/llama3-llava-next-8b-hf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. `make llama-llava-cli GGML_CUDA=1`\n",
    "2. gguf 파일이 해당 하드웨어에서 생성 불가 - 성능이슈(killed) https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README.md\n",
    "   -> huggingface에서 gguf 파일 다운로드 받음 \n",
    "   1. llava+llama3[llava-llama-3-8b-v1_1-int4.gguf(4.92 GB)]: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf\n",
    "   2. llava mistral[llava-v1.6-mistral-7b.Q4_K_M.gguf(4.37GB)]: https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf\n",
    "3. `./llama-llava-cli -m ../Llama3-Playground/models/llava-mistral-gguf/llava-v1.6-mistral-7b.Q4_K_M.gguf --mmproj ../Llama3-Playground/models/llava-mistral-gguf/mmproj-model-f16.gguf --image /home/jetson/cmap/athirdmapper/exp0610_ViT-B-16-SigLIP_3_copy/n_images/2.png -c 4096`\n",
    "   - `-p`(프롬프트 인수)의 사용법이 모델마다 다름 - https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README.md 페이지 참고 \n",
    "   - vicuna가 아닌 모든 모델은 system과 user 프롬프트를 지정해줘야 함 \n",
    "     - For Mistral and using llava-cli binary: Add this: `-p \"<image>\\nUSER:\\nProvide a full description.\\nASSISTANT:\\n\"`\n",
    "   - vicuna는 `-p \"Provide a full description.\"` 이와 같이 넣어주면됨 \n",
    "   - llama3은 `-p \"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\nDescribe this image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"`\n",
    "   - **구체적인 프롬프트는 robot_cmd.txt 에서 `llava` 검색하여 참고 바람 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llava-Llama3-8B\n",
    "\n",
    "Chat by llama.cpp\n",
    "    Build llama.cpp (docs) .\n",
    "    Build ./llava-cli (docs).\n",
    "\n",
    "Note: llava-llama-3-8b-v1_1 uses the Llama-3-instruct chat template.\n",
    "\n",
    "- fp16\n",
    "  - `./llava-cli -m ./llava-llama-3-8b-v1_1-f16.gguf --mmproj ./llava-llama-3-8b-v1_1-mmproj-f16.gguf --image YOUR_IMAGE.jpg -c 4096 -e -p \"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\nDescribe this image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"`\n",
    "\n",
    "- int4\n",
    "  - `./llava-cli -m ./llava-llama-3-8b-v1_1-int4.gguf --mmproj ./llava-llama-3-8b-v1_1-mmproj-f16.gguf --image YOUR_IMAGE.jpg -c 4096 -e -p \"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\nDescribe this image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"`\n",
    "\n",
    "\n",
    "https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama-cpp-python Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import  Llama"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
