[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  918][         clip_model_load] clip_model_load: loaded meta data with 25 key-value pairs and 378 tensors from models/llava-mistral-gguf/mmproj-model-f16.gguf
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  929][         clip_model_load] clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   0:                       general.architecture str              = clip
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   4:                          general.file_type u32              = 1
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   5:                               general.name str              = vit-large336-custom
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   7:                        clip.projector_type str              = mlp
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   8:                     clip.vision.image_size u32              = 336
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1024
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4096
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 768
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  15:                    clip.vision.block_count u32              = 23
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  16:           clip.vision.image_grid_pinpoints arr[i32,10]      = [336, 672, 672, 336, 672, 672, 1008, ...
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  17:          clip.vision.image_crop_resolution u32              = 224
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  18:             clip.vision.image_aspect_ratio str              = anyres
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  19:         clip.vision.image_split_resolution u32              = 224
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  20:            clip.vision.mm_patch_merge_type str              = spatial_unpad
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  21:              clip.vision.mm_projector_type str              = mlp2x_gelu
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  22:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  23:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  24:                              clip.use_gelu bool             = false
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  954][         clip_model_load] clip_model_load: - type  f32:  236 tensors
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  954][         clip_model_load] clip_model_load: - type  f16:  142 tensors
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  996][         clip_model_load] clip_model_load: CLIP using CUDA backend
[1722358703] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp: 1039][         clip_model_load] clip_model_load: params backend buffer size =  595.50 MB (378 tensors)
[1722358705] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp: 1316][         clip_model_load] clip_model_load: compute allocated memory: 32.89 MB
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  918][         clip_model_load] clip_model_load: loaded meta data with 25 key-value pairs and 378 tensors from models/llava-mistral-gguf/mmproj-model-f16.gguf
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  929][         clip_model_load] clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   0:                       general.architecture str              = clip
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   4:                          general.file_type u32              = 1
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   5:                               general.name str              = vit-large336-custom
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   7:                        clip.projector_type str              = mlp
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   8:                     clip.vision.image_size u32              = 336
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1024
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4096
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 768
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  15:                    clip.vision.block_count u32              = 23
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  16:           clip.vision.image_grid_pinpoints arr[i32,10]      = [336, 672, 672, 336, 672, 672, 1008, ...
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  17:          clip.vision.image_crop_resolution u32              = 224
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  18:             clip.vision.image_aspect_ratio str              = anyres
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  19:         clip.vision.image_split_resolution u32              = 224
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  20:            clip.vision.mm_patch_merge_type str              = spatial_unpad
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  21:              clip.vision.mm_projector_type str              = mlp2x_gelu
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  22:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  23:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  945][         clip_model_load] clip_model_load: - kv  24:                              clip.use_gelu bool             = false
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  954][         clip_model_load] clip_model_load: - type  f32:  236 tensors
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  954][         clip_model_load] clip_model_load: - type  f16:  142 tensors
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp:  996][         clip_model_load] clip_model_load: CLIP using CUDA backend
[1722358733] [/tmp/pip-install-6n504kbq/llama-cpp-python_3e292ba664124809b5c239a851c49922/vendor/llama.cpp/examples/llava/clip.cpp: 1039][         clip_model_load] clip_model_load: params backend buffer size =  595.50 MB (378 tensors)
